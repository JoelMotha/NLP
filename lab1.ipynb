{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize, TweetTokenizer, MWETokenizer, TreebankWordTokenizer\n",
    "import nltk.data\n",
    "import spacy\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize, TweetTokenizer, MWETokenizer, TreebankWordTokenizer\n",
    "from textblob import TextBlob\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Joel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ensure required resources are downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample text with emojis, punctuation, negation, and special characters\n",
    "text = \"\"\"AI-driven drones ğŸš€ğŸš€ are transforming industries! They don't just fly; they analyze, predict, and automate tasks effortlessly. \n",
    "Military applications, disaster management, and logisticsâ€”AI drones are revolutionizing everything. But... are we ready? ğŸ¤”ğŸ¤”\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Word Tokenization (NLTK)\n",
    "word_tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Sentence Tokenization (NLTK)\n",
    "sentence_tokens = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. Punctuation-based Tokenizer\n",
    "punct_tokens = regexp_tokenize(text, r\"\\w+|[^\\w\\s]\")  # Splits words but keeps punctuation as separate tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d. Treebank Word Tokenizer\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "treebank_tokens = treebank_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e. Tweet Tokenizer (handles emojis and hashtags well)\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokens = tweet_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f. Multi-Word Expression Tokenizer\n",
    "mwe_tokenizer = MWETokenizer([(\"AI\", \"drones\"), (\"disaster\", \"management\")])\n",
    "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g. TextBlob Word Tokenizer\n",
    "textblob_tokens = TextBlob(text).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h. spaCy Tokenizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_tokens = [token.text for token in nlp(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i. Gensim word tokenizer\n",
    "gensim_tokens = list(gensim.utils.tokenize(text, lower=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j. Tokenization with Keras\n",
    "keras_tokens = text_to_word_sequence(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization: ['AI-driven', 'drones', 'ğŸš€ğŸš€', 'are', 'transforming', 'industries', '!', 'They', 'do', \"n't\", 'just', 'fly', ';', 'they', 'analyze', ',', 'predict', ',', 'and', 'automate', 'tasks', 'effortlessly', '.', 'Military', 'applications', ',', 'disaster', 'management', ',', 'and', 'logisticsâ€”AI', 'drones', 'are', 'revolutionizing', 'everything', '.', 'But', '...', 'are', 'we', 'ready', '?', 'ğŸ¤”ğŸ¤”']\n",
      "Sentence Tokenization: ['AI-driven drones ğŸš€ğŸš€ are transforming industries!', \"They don't just fly; they analyze, predict, and automate tasks effortlessly.\", 'Military applications, disaster management, and logisticsâ€”AI drones are revolutionizing everything.', 'But... are we ready?', 'ğŸ¤”ğŸ¤”']\n",
      "Punctuation-based Tokenizer: ['AI', '-', 'driven', 'drones', 'ğŸš€', 'ğŸš€', 'are', 'transforming', 'industries', '!', 'They', 'don', \"'\", 't', 'just', 'fly', ';', 'they', 'analyze', ',', 'predict', ',', 'and', 'automate', 'tasks', 'effortlessly', '.', 'Military', 'applications', ',', 'disaster', 'management', ',', 'and', 'logistics', 'â€”', 'AI', 'drones', 'are', 'revolutionizing', 'everything', '.', 'But', '.', '.', '.', 'are', 'we', 'ready', '?', 'ğŸ¤”', 'ğŸ¤”']\n",
      "Treebank Word Tokenizer: ['AI-driven', 'drones', 'ğŸš€ğŸš€', 'are', 'transforming', 'industries', '!', 'They', 'do', \"n't\", 'just', 'fly', ';', 'they', 'analyze', ',', 'predict', ',', 'and', 'automate', 'tasks', 'effortlessly.', 'Military', 'applications', ',', 'disaster', 'management', ',', 'and', 'logisticsâ€”AI', 'drones', 'are', 'revolutionizing', 'everything.', 'But', '...', 'are', 'we', 'ready', '?', 'ğŸ¤”ğŸ¤”']\n",
      "Tweet Tokenizer: ['AI-driven', 'drones', 'ğŸš€', 'ğŸš€', 'are', 'transforming', 'industries', '!', 'They', \"don't\", 'just', 'fly', ';', 'they', 'analyze', ',', 'predict', ',', 'and', 'automate', 'tasks', 'effortlessly', '.', 'Military', 'applications', ',', 'disaster', 'management', ',', 'and', 'logistics', 'â€”', 'AI', 'drones', 'are', 'revolutionizing', 'everything', '.', 'But', '...', 'are', 'we', 'ready', '?', 'ğŸ¤”', 'ğŸ¤”']\n",
      "Multi-Word Expression Tokenizer: ['AI-driven', 'drones', 'ğŸš€ğŸš€', 'are', 'transforming', 'industries', '!', 'They', 'do', \"n't\", 'just', 'fly', ';', 'they', 'analyze', ',', 'predict', ',', 'and', 'automate', 'tasks', 'effortlessly', '.', 'Military', 'applications', ',', 'disaster_management', ',', 'and', 'logisticsâ€”AI', 'drones', 'are', 'revolutionizing', 'everything', '.', 'But', '...', 'are', 'we', 'ready', '?', 'ğŸ¤”ğŸ¤”']\n",
      "TextBlob Tokenizer: ['AI-driven', 'drones', 'ğŸš€ğŸš€', 'are', 'transforming', 'industries', 'They', 'do', \"n't\", 'just', 'fly', 'they', 'analyze', 'predict', 'and', 'automate', 'tasks', 'effortlessly', 'Military', 'applications', 'disaster', 'management', 'and', 'logisticsâ€”AI', 'drones', 'are', 'revolutionizing', 'everything', 'But', 'are', 'we', 'ready', 'ğŸ¤”ğŸ¤”']\n",
      "spaCy Tokenizer: ['AI', '-', 'driven', 'drones', 'ğŸš€', 'ğŸš€', 'are', 'transforming', 'industries', '!', 'They', 'do', \"n't\", 'just', 'fly', ';', 'they', 'analyze', ',', 'predict', ',', 'and', 'automate', 'tasks', 'effortlessly', '.', '\\n', 'Military', 'applications', ',', 'disaster', 'management', ',', 'and', 'logistics', 'â€”', 'AI', 'drones', 'are', 'revolutionizing', 'everything', '.', 'But', '...', 'are', 'we', 'ready', '?', 'ğŸ¤”', 'ğŸ¤”']\n",
      "Gensim Tokenizer: ['ai', 'driven', 'drones', 'are', 'transforming', 'industries', 'they', 'don', 't', 'just', 'fly', 'they', 'analyze', 'predict', 'and', 'automate', 'tasks', 'effortlessly', 'military', 'applications', 'disaster', 'management', 'and', 'logistics', 'ai', 'drones', 'are', 'revolutionizing', 'everything', 'but', 'are', 'we', 'ready']\n",
      "Keras Tokenizer: ['ai', 'driven', 'drones', 'ğŸš€ğŸš€', 'are', 'transforming', 'industries', 'they', \"don't\", 'just', 'fly', 'they', 'analyze', 'predict', 'and', 'automate', 'tasks', 'effortlessly', 'military', 'applications', 'disaster', 'management', 'and', 'logisticsâ€”ai', 'drones', 'are', 'revolutionizing', 'everything', 'but', 'are', 'we', 'ready', 'ğŸ¤”ğŸ¤”']\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "print(\"Word Tokenization:\", word_tokens)\n",
    "print(\"Sentence Tokenization:\", sentence_tokens)\n",
    "print(\"Punctuation-based Tokenizer:\", punct_tokens)\n",
    "print(\"Treebank Word Tokenizer:\", treebank_tokens)\n",
    "print(\"Tweet Tokenizer:\", tweet_tokens)\n",
    "print(\"Multi-Word Expression Tokenizer:\", mwe_tokens)\n",
    "print(\"TextBlob Tokenizer:\", list(textblob_tokens))\n",
    "print(\"spaCy Tokenizer:\", spacy_tokens)\n",
    "print(\"Gensim Tokenizer:\", gensim_tokens)\n",
    "print(\"Keras Tokenizer:\", keras_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1ï¸. Word Tokenization**\n",
    "\n",
    "ğŸ”¹ Insight:\n",
    "\n",
    "Splits text into individual words.\n",
    "\n",
    "Handles spaces, but may struggle with contractions and punctuation.\n",
    "\n",
    "ğŸ”¹ Applications:\n",
    "\n",
    "Sentiment Analysis (Identifying positive/negative words).\n",
    "\n",
    "Information Retrieval (Extracting keywords).\n",
    "\n",
    "Word Frequency Analysis (For text mining & corpus analysis).\n",
    "\n",
    "**2. Sentence Tokenization**\n",
    "\n",
    "ğŸ”¹ Insight:\n",
    "\n",
    "Splits text into sentences using punctuation like \".\", \"!\", and \"?\".\n",
    "\n",
    "Some languages (e.g., Chinese) donâ€™t have clear sentence boundaries, making this tricky.\n",
    "\n",
    "ğŸ”¹ Applications:\n",
    "\n",
    "Text Summarization (Splitting long documents into meaningful chunks).\n",
    "\n",
    "Question-Answering Systems (Breaking text into manageable responses).\n",
    "\n",
    "Chatbots (Processing sentences independently).\n",
    "\n",
    "**3. Punctuation-Based Tokenizer**\n",
    "\n",
    "ğŸ”¹ Insight:\n",
    "\n",
    "Splits text based on punctuation marks.\n",
    "\n",
    "Useful for detailed text analysis, but might split meaningful entities.\n",
    "\n",
    "ğŸ”¹ Applications:\n",
    "\n",
    "Processing Code & Logs (Where punctuation has meaning).\n",
    "\n",
    "Text Cleaning (Removing unnecessary punctuation from text).\n",
    "\n",
    "Text Compression (Removing redundant punctuation).\n",
    "\n",
    "**4. Treebank Word Tokenizer**\n",
    "\n",
    "ğŸ”¹ Insight:\n",
    "\n",
    "Uses Penn Treebank rules for splitting text.\n",
    "\n",
    "Handles contractions (e.g., donâ€™t â†’ do + n't).\n",
    "\n",
    "ğŸ”¹ Applications:\n",
    "\n",
    "Part-of-Speech Tagging (Breaking words properly for linguistic parsing).\n",
    "\n",
    "Named Entity Recognition (NER) (More accurate entity extraction).\n",
    "\n",
    "Parsing Text for Syntax Analysis (Used in deep NLP models).\n",
    "\n",
    "**5. Tweet Tokenizer**\n",
    "\n",
    "ğŸ”¹ Insight:\n",
    "\n",
    "Special tokenizer for social media text (handles hashtags, emojis, mentions).\n",
    "\n",
    "Avoids breaking URLs and special symbols.\n",
    "\n",
    "ğŸ”¹ Applications:\n",
    "\n",
    "Social Media Sentiment Analysis (Extracting meaningful words from tweets/posts).\n",
    "\n",
    "Fake News Detection (Analyzing text patterns in social media).\n",
    "\n",
    "Hashtag and Mention Analysis (Tracking trends).\n",
    "\n",
    "**6. Multi-Word Expression Tokenizer**\n",
    "\n",
    "ğŸ”¹ Insight:\n",
    "\n",
    "Recognizes multi-word expressions like New York, machine learning, data science.\n",
    "\n",
    "Uses predefined phrases or statistical methods to detect them.\n",
    "\n",
    "ğŸ”¹ Applications:\n",
    "\n",
    "Information Extraction (Ensuring terms like \"artificial intelligence\" arenâ€™t split).\n",
    "\n",
    "Named Entity Recognition (NER) (Handling multi-word entity names).\n",
    "\n",
    "Machine Translation (Preserving phrase meanings).\n",
    "\n",
    "**7. TextBlob Word Tokenizer**\n",
    "\n",
    "ğŸ”¹ Insight:\n",
    "\n",
    "Uses TextBlob's NLP engine to tokenize text efficiently.\n",
    "\n",
    "Simple, accurate, and widely used in beginner-level NLP tasks.\n",
    "\n",
    "ğŸ”¹ Applications:\n",
    "\n",
    "Sentiment Analysis (Often used with TextBlob's built-in sentiment tools).\n",
    "\n",
    "Spelling Correction (Tokenization is a preprocessing step).\n",
    "\n",
    "Basic Chatbot Development (Easy and fast processing).\n",
    "\n",
    "**8. spaCy Tokenizer**\n",
    "\n",
    "ğŸ”¹ Insight:\n",
    "\n",
    "Highly efficient, optimized tokenizer that works well for large-scale NLP.\n",
    "\n",
    "Handles complex linguistic rules automatically.\n",
    "\n",
    "ğŸ”¹ Applications:\n",
    "\n",
    "Large-Scale NLP (Fast and accurate processing of big text data).\n",
    "\n",
    "Dependency Parsing (Understanding sentence structure).\n",
    "\n",
    "Legal/Medical Text Analysis (Extracting meaningful content).\n",
    "\n",
    "**9. Gensim Word Tokenizer**\n",
    "\n",
    "ğŸ”¹ Insight:\n",
    "\n",
    "Designed for tokenizing text for topic modeling and document similarity.\n",
    "\n",
    "Works well with word embeddings and vectorization.\n",
    "\n",
    "ğŸ”¹ Applications:\n",
    "\n",
    "Topic Modeling (LDA, Word2Vec, Doc2Vec).\n",
    "\n",
    "Document Similarity Search (Used in search engines).\n",
    "\n",
    "Text Clustering (Grouping similar documents).\n",
    "\n",
    "**10. Tokenization with Keras**\n",
    "\n",
    "ğŸ”¹ Insight:\n",
    "\n",
    "Prepares text for deep learning models in Keras and TensorFlow.\n",
    "\n",
    "Converts words into sequences (IDs) for embedding layers.\n",
    "\n",
    "ğŸ”¹ Applications:\n",
    "\n",
    "Training NLP models (RNNs, LSTMs, Transformers).\n",
    "\n",
    "Text Classification (Spam detection, emotion recognition).\n",
    "\n",
    "Chatbot Training (Handling large datasets efficiently)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
