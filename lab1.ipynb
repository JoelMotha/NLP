{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize, TweetTokenizer, MWETokenizer, TreebankWordTokenizer\n",
    "import nltk.data\n",
    "import spacy\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize, TweetTokenizer, MWETokenizer, TreebankWordTokenizer\n",
    "from textblob import TextBlob\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "# Ensure required resources are downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample text with emojis, punctuation, negation, and special characters\n",
    "text = \"\"\"John's dog doesn't like playing outside; however, it enjoys running‚Äîespecially in the morning! Also, $50 isn't too much for a toy, right?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Word Tokenization (NLTK)\n",
    "word_tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Sentence Tokenization (NLTK)\n",
    "sentence_tokens = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. Punctuation-based Tokenizer\n",
    "punct_tokens = regexp_tokenize(text, r\"\\w+|[^\\w\\s]\")  # Splits words but keeps punctuation as separate tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d. Treebank Word Tokenizer\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "treebank_tokens = treebank_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e. Tweet Tokenizer (handles emojis and hashtags well)\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokens = tweet_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f. Multi-Word Expression Tokenizer\n",
    "mwe_tokenizer = MWETokenizer([(\"AI\", \"drones\"), (\"disaster\", \"management\")])\n",
    "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g. TextBlob Word Tokenizer\n",
    "textblob_tokens = TextBlob(text).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h. spaCy Tokenizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_tokens = [token.text for token in nlp(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i. Gensim word tokenizer\n",
    "gensim_tokens = list(gensim.utils.tokenize(text, lower=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j. Tokenization with Keras\n",
    "keras_tokens = text_to_word_sequence(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k. Includes the words with apostrophe\n",
    "whitespace_tokens = text.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization: ['John', \"'s\", 'dog', 'does', \"n't\", 'like', 'playing', 'outside', ';', 'however', ',', 'it', 'enjoys', 'running‚Äîespecially', 'in', 'the', 'morning', '!', 'Also', ',', '$', '50', 'is', \"n't\", 'too', 'much', 'for', 'a', 'toy', ',', 'right', '?']\n",
      "Sentence Tokenization: [\"John's dog doesn't like playing outside; however, it enjoys running‚Äîespecially in the morning!\", \"Also, $50 isn't too much for a toy, right?\"]\n",
      "Punctuation-based Tokenizer: ['John', \"'\", 's', 'dog', 'doesn', \"'\", 't', 'like', 'playing', 'outside', ';', 'however', ',', 'it', 'enjoys', 'running', '‚Äî', 'especially', 'in', 'the', 'morning', '!', 'Also', ',', '$', '50', 'isn', \"'\", 't', 'too', 'much', 'for', 'a', 'toy', ',', 'right', '?']\n",
      "Treebank Word Tokenizer: ['John', \"'s\", 'dog', 'does', \"n't\", 'like', 'playing', 'outside', ';', 'however', ',', 'it', 'enjoys', 'running‚Äîespecially', 'in', 'the', 'morning', '!', 'Also', ',', '$', '50', 'is', \"n't\", 'too', 'much', 'for', 'a', 'toy', ',', 'right', '?']\n",
      "Tweet Tokenizer: [\"John's\", 'dog', \"doesn't\", 'like', 'playing', 'outside', ';', 'however', ',', 'it', 'enjoys', 'running', '‚Äî', 'especially', 'in', 'the', 'morning', '!', 'Also', ',', '$', '50', \"isn't\", 'too', 'much', 'for', 'a', 'toy', ',', 'right', '?']\n",
      "Multi-Word Expression Tokenizer: ['John', \"'s\", 'dog', 'does', \"n't\", 'like', 'playing', 'outside', ';', 'however', ',', 'it', 'enjoys', 'running‚Äîespecially', 'in', 'the', 'morning', '!', 'Also', ',', '$', '50', 'is', \"n't\", 'too', 'much', 'for', 'a', 'toy', ',', 'right', '?']\n",
      "TextBlob Tokenizer: ['John', \"'s\", 'dog', 'does', \"n't\", 'like', 'playing', 'outside', 'however', 'it', 'enjoys', 'running‚Äîespecially', 'in', 'the', 'morning', 'Also', '50', 'is', \"n't\", 'too', 'much', 'for', 'a', 'toy', 'right']\n",
      "spaCy Tokenizer: ['John', \"'s\", 'dog', 'does', \"n't\", 'like', 'playing', 'outside', ';', 'however', ',', 'it', 'enjoys', 'running', '‚Äî', 'especially', 'in', 'the', 'morning', '!', 'Also', ',', '$', '50', 'is', \"n't\", 'too', 'much', 'for', 'a', 'toy', ',', 'right', '?']\n",
      "Gensim Tokenizer: ['john', 's', 'dog', 'doesn', 't', 'like', 'playing', 'outside', 'however', 'it', 'enjoys', 'running', 'especially', 'in', 'the', 'morning', 'also', 'isn', 't', 'too', 'much', 'for', 'a', 'toy', 'right']\n",
      "Keras Tokenizer: [\"john's\", 'dog', \"doesn't\", 'like', 'playing', 'outside', 'however', 'it', 'enjoys', 'running‚Äîespecially', 'in', 'the', 'morning', 'also', '50', \"isn't\", 'too', 'much', 'for', 'a', 'toy', 'right']\n",
      "Whitespace Tokenizer: [\"John's\", 'dog', \"doesn't\", 'like', 'playing', 'outside;', 'however,', 'it', 'enjoys', 'running‚Äîespecially', 'in', 'the', 'morning!', 'Also,', '$50', \"isn't\", 'too', 'much', 'for', 'a', 'toy,', 'right?']\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "print(\"Word Tokenization:\", word_tokens)\n",
    "print(\"Sentence Tokenization:\", sentence_tokens)\n",
    "print(\"Punctuation-based Tokenizer:\", punct_tokens)\n",
    "print(\"Treebank Word Tokenizer:\", treebank_tokens)\n",
    "print(\"Tweet Tokenizer:\", tweet_tokens)\n",
    "print(\"Multi-Word Expression Tokenizer:\", mwe_tokens)\n",
    "print(\"TextBlob Tokenizer:\", list(textblob_tokens))\n",
    "print(\"spaCy Tokenizer:\", spacy_tokens)\n",
    "print(\"Gensim Tokenizer:\", gensim_tokens)\n",
    "print(\"Keras Tokenizer:\", keras_tokens)\n",
    "print(\"Whitespace Tokenizer:\", whitespace_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1Ô∏è. Word Tokenization**\n",
    "\n",
    "üîπ Insight:\n",
    "\n",
    "Splits text into individual words.\n",
    "\n",
    "Handles spaces, but may struggle with contractions and punctuation.\n",
    "\n",
    "üîπ Applications:\n",
    "\n",
    "Sentiment Analysis (Identifying positive/negative words).\n",
    "\n",
    "Information Retrieval (Extracting keywords).\n",
    "\n",
    "Word Frequency Analysis (For text mining & corpus analysis).\n",
    "\n",
    "**2. Sentence Tokenization**\n",
    "\n",
    "üîπ Insight:\n",
    "\n",
    "Splits text into sentences using punctuation like \".\", \"!\", and \"?\".\n",
    "\n",
    "Some languages (e.g., Chinese) don‚Äôt have clear sentence boundaries, making this tricky.\n",
    "\n",
    "üîπ Applications:\n",
    "\n",
    "Text Summarization (Splitting long documents into meaningful chunks).\n",
    "\n",
    "Question-Answering Systems (Breaking text into manageable responses).\n",
    "\n",
    "Chatbots (Processing sentences independently).\n",
    "\n",
    "**3. Punctuation-Based Tokenizer**\n",
    "\n",
    "üîπ Insight:\n",
    "\n",
    "Splits text based on punctuation marks.\n",
    "\n",
    "Useful for detailed text analysis, but might split meaningful entities.\n",
    "\n",
    "üîπ Applications:\n",
    "\n",
    "Processing Code & Logs (Where punctuation has meaning).\n",
    "\n",
    "Text Cleaning (Removing unnecessary punctuation from text).\n",
    "\n",
    "Text Compression (Removing redundant punctuation).\n",
    "\n",
    "**4. Treebank Word Tokenizer**\n",
    "\n",
    "üîπ Insight:\n",
    "\n",
    "Uses Penn Treebank rules for splitting text.\n",
    "\n",
    "Handles contractions (e.g., don‚Äôt ‚Üí do + n't).\n",
    "\n",
    "üîπ Applications:\n",
    "\n",
    "Part-of-Speech Tagging (Breaking words properly for linguistic parsing).\n",
    "\n",
    "Named Entity Recognition (NER) (More accurate entity extraction).\n",
    "\n",
    "Parsing Text for Syntax Analysis (Used in deep NLP models).\n",
    "\n",
    "**5. Tweet Tokenizer**\n",
    "\n",
    "üîπ Insight:\n",
    "\n",
    "Special tokenizer for social media text (handles hashtags, emojis, mentions).\n",
    "\n",
    "Avoids breaking URLs and special symbols.\n",
    "\n",
    "üîπ Applications:\n",
    "\n",
    "Social Media Sentiment Analysis (Extracting meaningful words from tweets/posts).\n",
    "\n",
    "Fake News Detection (Analyzing text patterns in social media).\n",
    "\n",
    "Hashtag and Mention Analysis (Tracking trends).\n",
    "\n",
    "**6. Multi-Word Expression Tokenizer**\n",
    "\n",
    "üîπ Insight:\n",
    "\n",
    "Recognizes multi-word expressions like New York, machine learning, data science.\n",
    "\n",
    "Uses predefined phrases or statistical methods to detect them.\n",
    "\n",
    "üîπ Applications:\n",
    "\n",
    "Information Extraction (Ensuring terms like \"artificial intelligence\" aren‚Äôt split).\n",
    "\n",
    "Named Entity Recognition (NER) (Handling multi-word entity names).\n",
    "\n",
    "Machine Translation (Preserving phrase meanings).\n",
    "\n",
    "**7. TextBlob Word Tokenizer**\n",
    "\n",
    "üîπ Insight:\n",
    "\n",
    "Uses TextBlob's NLP engine to tokenize text efficiently.\n",
    "\n",
    "Simple, accurate, and widely used in beginner-level NLP tasks.\n",
    "\n",
    "üîπ Applications:\n",
    "\n",
    "Sentiment Analysis (Often used with TextBlob's built-in sentiment tools).\n",
    "\n",
    "Spelling Correction (Tokenization is a preprocessing step).\n",
    "\n",
    "Basic Chatbot Development (Easy and fast processing).\n",
    "\n",
    "**8. spaCy Tokenizer**\n",
    "\n",
    "üîπ Insight:\n",
    "\n",
    "Highly efficient, optimized tokenizer that works well for large-scale NLP.\n",
    "\n",
    "Handles complex linguistic rules automatically.\n",
    "\n",
    "üîπ Applications:\n",
    "\n",
    "Large-Scale NLP (Fast and accurate processing of big text data).\n",
    "\n",
    "Dependency Parsing (Understanding sentence structure).\n",
    "\n",
    "Legal/Medical Text Analysis (Extracting meaningful content).\n",
    "\n",
    "**9. Gensim Word Tokenizer**\n",
    "\n",
    "üîπ Insight:\n",
    "\n",
    "Designed for tokenizing text for topic modeling and document similarity.\n",
    "\n",
    "Works well with word embeddings and vectorization.\n",
    "\n",
    "üîπ Applications:\n",
    "\n",
    "Topic Modeling (LDA, Word2Vec, Doc2Vec).\n",
    "\n",
    "Document Similarity Search (Used in search engines).\n",
    "\n",
    "Text Clustering (Grouping similar documents).\n",
    "\n",
    "**10. Tokenization with Keras**\n",
    "\n",
    "üîπ Insight:\n",
    "\n",
    "Prepares text for deep learning models in Keras and TensorFlow.\n",
    "\n",
    "Converts words into sequences (IDs) for embedding layers.\n",
    "\n",
    "üîπ Applications:\n",
    "\n",
    "Training NLP models (RNNs, LSTMs, Transformers).\n",
    "\n",
    "Text Classification (Spam detection, emotion recognition).\n",
    "\n",
    "Chatbot Training (Handling large datasets efficiently)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
