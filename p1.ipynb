{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mKeyError\u001b[0m: 'nltk'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.modules['nltk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: nltk\n",
      "Version: 3.8.1\n",
      "Summary: Natural Language Toolkit\n",
      "Home-page: https://www.nltk.org/\n",
      "Author: NLTK Team\n",
      "Author-email: nltk.team@gmail.com\n",
      "License: Apache License, Version 2.0\n",
      "Location: c:\\Users\\Joel\\anaconda3\\Lib\\site-packages\n",
      "Requires: click, joblib, regex, tqdm\n",
      "Required-by: textblob\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\joel\\anaconda3\\lib\\site-packages (3.8.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: click in c:\\users\\joel\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\joel\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\joel\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\joel\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "pip install --no-cache-dir nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.1\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Joel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m textblob_tokens \u001b[38;5;241m=\u001b[39m TextBlob(text)\u001b[38;5;241m.\u001b[39mwords\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# h. spaCy Tokenizer\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m spacy_tokens \u001b[38;5;241m=\u001b[39m [token\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m nlp(text)]\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# i. Gensim word tokenizer\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Joel\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m util\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m     52\u001b[0m         name,\n\u001b[0;32m     53\u001b[0m         vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[0;32m     54\u001b[0m         disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[0;32m     55\u001b[0m         enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[0;32m     56\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[0;32m     57\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m     58\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Joel\\anaconda3\\Lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize, TweetTokenizer, MWETokenizer, TreebankWordTokenizer\n",
    "import nltk.data\n",
    "import spacy\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize, TweetTokenizer, MWETokenizer, TreebankWordTokenizer\n",
    "from textblob import TextBlob\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "\n",
    "# Ensure required resources are downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample text with emojis, punctuation, negation, and special characters\n",
    "text = \"\"\"AI-driven drones ðŸš€ðŸš€ are transforming industries! They don't just fly; they analyze, predict, and automate tasks effortlessly. \n",
    "Military applications, disaster management, and logisticsâ€”AI drones are revolutionizing everything. But... are we ready? ðŸ¤”ðŸ¤”\"\"\"\n",
    "\n",
    "# a. Word Tokenization (NLTK)\n",
    "word_tokens = word_tokenize(text)\n",
    "\n",
    "# b. Sentence Tokenization (NLTK)\n",
    "sentence_tokens = sent_tokenize(text)\n",
    "\n",
    "# c. Punctuation-based Tokenizer\n",
    "punct_tokens = regexp_tokenize(text, r\"\\w+|[^\\w\\s]\")  # Splits words but keeps punctuation as separate tokens\n",
    "\n",
    "# d. Treebank Word Tokenizer\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "treebank_tokens = treebank_tokenizer.tokenize(text)\n",
    "\n",
    "# e. Tweet Tokenizer (handles emojis and hashtags well)\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokens = tweet_tokenizer.tokenize(text)\n",
    "\n",
    "# f. Multi-Word Expression Tokenizer\n",
    "mwe_tokenizer = MWETokenizer([(\"AI\", \"drones\"), (\"disaster\", \"management\")])\n",
    "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(text))\n",
    "\n",
    "# g. TextBlob Word Tokenizer\n",
    "textblob_tokens = TextBlob(text).words\n",
    "\n",
    "# h. spaCy Tokenizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_tokens = [token.text for token in nlp(text)]\n",
    "\n",
    "# i. Gensim word tokenizer\n",
    "gensim_tokens = list(gensim.utils.tokenize(text, lower=True))\n",
    "\n",
    "# j. Tokenization with Keras\n",
    "keras_tokens = text_to_word_sequence(text)\n",
    "\n",
    "# Display results\n",
    "print(\"Word Tokenization:\", word_tokens)\n",
    "print(\"Sentence Tokenization:\", sentence_tokens)\n",
    "print(\"Punctuation-based Tokenizer:\", punct_tokens)\n",
    "print(\"Treebank Word Tokenizer:\", treebank_tokens)\n",
    "print(\"Tweet Tokenizer:\", tweet_tokens)\n",
    "print(\"Multi-Word Expression Tokenizer:\", mwe_tokens)\n",
    "print(\"TextBlob Tokenizer:\", list(textblob_tokens))\n",
    "print(\"spaCy Tokenizer:\", spacy_tokens)\n",
    "print(\"Gensim Tokenizer:\", gensim_tokens)\n",
    "print(\"Keras Tokenizer:\", keras_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0.tar.gz (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 217.9 kB/s eta 0:00:59\n",
      "     --------------------------------------- 0.0/12.8 MB 217.9 kB/s eta 0:00:59\n",
      "     --------------------------------------- 0.1/12.8 MB 525.1 kB/s eta 0:00:25\n",
      "      --------------------------------------- 0.3/12.8 MB 1.2 MB/s eta 0:00:11\n",
      "     -- ------------------------------------- 0.7/12.8 MB 2.7 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 1.5/12.8 MB 4.9 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 2.6/12.8 MB 7.5 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 8.7 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 4.0/12.8 MB 9.4 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 4.0/12.8 MB 9.4 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 4.0/12.8 MB 9.4 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 4.9/12.8 MB 8.7 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 5.0/12.8 MB 8.6 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 6.8/12.8 MB 10.3 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 13.1 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.4/12.8 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.5/12.8 MB 17.7 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 11.1/12.8 MB 17.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 18.2 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 17.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting spacy<3.6.0,>=3.5.0 (from en-core-web-sm==3.5.0)\n",
      "  Downloading spacy-3.5.4-cp311-cp311-win_amd64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.9)\n",
      "Collecting thinc<8.2.0,>=8.1.8 (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0)\n",
      "  Downloading thinc-8.1.12-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.10)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0)\n",
      "  Downloading typer-0.9.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting pathy>=0.10.0 (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0)\n",
      "  Downloading pathy-0.11.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\joel\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.3.0)\n",
      "Collecting pathlib-abc==0.1.1 (from pathy>=0.10.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0)\n",
      "  Downloading pathlib_abc-0.1.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2024.8.30)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0)\n",
      "  Downloading blis-0.7.11-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\joel\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\joel\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.2.1)\n",
      "Downloading spacy-3.5.4-cp311-cp311-win_amd64.whl (12.2 MB)\n",
      "   ---------------------------------------- 0.0/12.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.2 MB 991.0 kB/s eta 0:00:13\n",
      "    --------------------------------------- 0.2/12.2 MB 2.1 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.5/12.2 MB 3.9 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.0/12.2 MB 5.9 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 1.0/12.2 MB 6.0 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 1.0/12.2 MB 6.0 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.6/12.2 MB 5.2 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.1/12.2 MB 6.3 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.1/12.2 MB 6.3 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.1/12.2 MB 6.3 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.1/12.2 MB 6.3 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.1/12.2 MB 3.9 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.9/12.2 MB 4.9 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.1/12.2 MB 4.9 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.1/12.2 MB 4.9 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.1/12.2 MB 4.9 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.3/12.2 MB 4.3 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/12.2 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 4.2/12.2 MB 4.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.2/12.2 MB 4.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.2/12.2 MB 4.8 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.5/12.2 MB 4.5 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.6/12.2 MB 4.3 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/12.2 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/12.2 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/12.2 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/12.2 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/12.2 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/12.2 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/12.2 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/12.2 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/12.2 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/12.2 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/12.2 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/12.2 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/12.2 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/12.2 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.4/12.2 MB 3.1 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 6.2/12.2 MB 3.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.3/12.2 MB 3.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.3/12.2 MB 3.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.3/12.2 MB 3.5 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.8/12.2 MB 3.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.9/12.2 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.3/12.2 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.3/12.2 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.3/12.2 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.3/12.2 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.3/12.2 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.7/12.2 MB 3.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.4/12.2 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.4/12.2 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.4/12.2 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.4/12.2 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.4/12.2 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.4/12.2 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.4/12.2 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.4/12.2 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.4/12.2 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.4/12.2 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.4/12.2 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.4/12.2 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.4/12.2 MB 3.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 8.6/12.2 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 9.4/12.2 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.4/12.2 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.4/12.2 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.4/12.2 MB 3.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.2/12.2 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.5/12.2 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.5/12.2 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.5/12.2 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.5/12.2 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.0/12.2 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.8/12.2 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.2/12.2 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.2/12.2 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.2/12.2 MB 3.1 MB/s eta 0:00:00\n",
      "Downloading pathy-0.11.0-py3-none-any.whl (47 kB)\n",
      "   ---------------------------------------- 0.0/47.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 47.3/47.3 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading pathlib_abc-0.1.1-py3-none-any.whl (23 kB)\n",
      "Downloading thinc-8.1.12-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 0.7/1.5 MB 14.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 18.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 11.7 MB/s eta 0:00:00\n",
      "Downloading typer-0.9.4-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/46.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 46.0/46.0 kB 1.2 MB/s eta 0:00:00\n",
      "Downloading blis-0.7.11-cp311-cp311-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.8/6.6 MB 16.6 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.8/6.6 MB 18.5 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 1.9/6.6 MB 13.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.7/6.6 MB 14.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.6/6.6 MB 19.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.4/6.6 MB 20.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.6/6.6 MB 19.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 18.3 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py): started\n",
      "  Building wheel for en-core-web-sm (setup.py): finished with status 'done'\n",
      "  Created wheel for en-core-web-sm: filename=en_core_web_sm-3.5.0-py3-none-any.whl size=12803295 sha256=9617d4ae73d8bf29afb3cb0f9b0f9d0cf73ba445135e4cba7597ccf6113490d8\n",
      "  Stored in directory: c:\\users\\joel\\appdata\\local\\pip\\cache\\wheels\\82\\46\\d9\\f2f2545558f7b9013b2c9cf95c96c5f332ed581f279b2f6a03\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: pathlib-abc, blis, typer, thinc, pathy, spacy, en-core-web-sm\n",
      "  Attempting uninstall: blis\n",
      "    Found existing installation: blis 1.2.0\n",
      "    Uninstalling blis-1.2.0:\n",
      "      Successfully uninstalled blis-1.2.0\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.15.1\n",
      "    Uninstalling typer-0.15.1:\n",
      "      Successfully uninstalled typer-0.15.1\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.3.4\n",
      "    Uninstalling thinc-8.3.4:\n",
      "      Successfully uninstalled thinc-8.3.4\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.8.4\n",
      "    Uninstalling spacy-3.8.4:\n",
      "      Successfully uninstalled spacy-3.8.4\n",
      "Successfully installed blis-0.7.11 en-core-web-sm-3.5.0 pathlib-abc-0.1.1 pathy-0.11.0 spacy-3.5.4 thinc-8.1.12 typer-0.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Joel\\anaconda3\\Lib\\site-packages\\~lis'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Joel\\anaconda3\\Lib\\site-packages\\~hinc'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Joel\\anaconda3\\Lib\\site-packages\\~pacy'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize, TweetTokenizer, MWETokenizer, TreebankWordTokenizer\n",
    "import nltk.data\n",
    "import spacy\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize, TweetTokenizer, MWETokenizer, TreebankWordTokenizer\n",
    "from textblob import TextBlob\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "\n",
    "# Ensure required resources are downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample text with emojis, punctuation, negation, and special characters\n",
    "text = \"\"\"AI-driven drones ðŸš€ðŸš€ are transforming industries! They don't just fly; they analyze, predict, and automate tasks effortlessly. \n",
    "Military applications, disaster management, and logisticsâ€”AI drones are revolutionizing everything. But... are we ready? ðŸ¤”ðŸ¤”\"\"\"\n",
    "\n",
    "# a. Word Tokenization (NLTK)\n",
    "word_tokens = word_tokenize(text)\n",
    "\n",
    "# b. Sentence Tokenization (NLTK)\n",
    "sentence_tokens = sent_tokenize(text)\n",
    "\n",
    "# c. Punctuation-based Tokenizer\n",
    "punct_tokens = regexp_tokenize(text, r\"\\w+|[^\\w\\s]\")  # Splits words but keeps punctuation as separate tokens\n",
    "\n",
    "# d. Treebank Word Tokenizer\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "treebank_tokens = treebank_tokenizer.tokenize(text)\n",
    "\n",
    "# e. Tweet Tokenizer (handles emojis and hashtags well)\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokens = tweet_tokenizer.tokenize(text)\n",
    "\n",
    "# f. Multi-Word Expression Tokenizer\n",
    "mwe_tokenizer = MWETokenizer([(\"AI\", \"drones\"), (\"disaster\", \"management\")])\n",
    "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(text))\n",
    "\n",
    "# g. TextBlob Word Tokenizer\n",
    "textblob_tokens = TextBlob(text).words\n",
    "\n",
    "# h. spaCy Tokenizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_tokens = [token.text for token in nlp(text)]\n",
    "\n",
    "# i. Gensim word tokenizer\n",
    "gensim_tokens = list(gensim.utils.tokenize(text, lower=True))\n",
    "\n",
    "# j. Tokenization with Keras\n",
    "keras_tokens = text_to_word_sequence(text)\n",
    "\n",
    "# Display results\n",
    "print(\"Word Tokenization:\", word_tokens)\n",
    "print(\"Sentence Tokenization:\", sentence_tokens)\n",
    "print(\"Punctuation-based Tokenizer:\", punct_tokens)\n",
    "print(\"Treebank Word Tokenizer:\", treebank_tokens)\n",
    "print(\"Tweet Tokenizer:\", tweet_tokens)\n",
    "print(\"Multi-Word Expression Tokenizer:\", mwe_tokens)\n",
    "print(\"TextBlob Tokenizer:\", list(textblob_tokens))\n",
    "print(\"spaCy Tokenizer:\", spacy_tokens)\n",
    "print(\"Gensim Tokenizer:\", gensim_tokens)\n",
    "print(\"Keras Tokenizer:\", keras_tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
